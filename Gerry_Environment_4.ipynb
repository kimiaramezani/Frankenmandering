{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Gerrymandering-Environment**\n",
        "\n",
        "    INITIAL STATE (provided externally via reset(options=...)):\n",
        "        - 'district_map'\n",
        "        - 'social_graph'\n",
        "        - 'opinions'     \n",
        "\n",
        "    ACTION:\n",
        "        - new district assignment for each voter\n",
        "\n",
        "    OBSERVATION (returned by reset/step):\n",
        "        {\n",
        "          'district_map'   : (num_voters,)\n",
        "          'representatives': (num_districts,)  # voter indices; -1 if empty district\n",
        "          'social_graph'   : (num_voters, num_voters)  # AUGMENTED: base social + rep->voter edges used for the step\n",
        "          'opinions'       : (num_voters, 2)\n",
        "          'opinion_graph'  : (num_voters, num_voters)  # similarity kernel derived from opinion distances\n",
        "        }\n",
        "\n",
        "    KEY LOGIC:\n",
        "      - Representatives: for each district, pick the member that minimizes the sum of L2 distances to members in that district (discrete 1-median).\n",
        "      - Opinion dynamics: DRF (assimilation/neutral/backfire) with weighted neighbor influence.\n",
        "      - Reward: reduction in total distance to reference opinion c*\n",
        "\n",
        "    Notes:\n",
        "      - Opinion weight = 1\n",
        "      - Opinion dimension is fixed at 2.\n",
        "      - We accept any districting action"
      ],
      "metadata": {
        "id": "YVPu0WeaABON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7PJyuKxNG26",
        "outputId": "68e3b55e-44bd-4286-bdae-05ed3b9c3de2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces"
      ],
      "metadata": {
        "id": "ErlKBfKcRnP7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FrankenmanderingEnv(gym.Env):\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_voters: int,\n",
        "        num_districts: int,\n",
        "        opinion_dim: int = 2,\n",
        "        horizon: int = 10,\n",
        "        seed: int | None = None,\n",
        "\n",
        "        # opinion dynamics\n",
        "        eta: float = 0.2,\n",
        "        a_thresh: float = 0.4,\n",
        "        b_thresh: float = 1.2,\n",
        "        mu_assim: float = 1.0,\n",
        "        mu_backfire: float = -0.5,\n",
        "\n",
        "        # representative influence\n",
        "        rep_edge_weight: float = 1.0,\n",
        "\n",
        "        # reward target\n",
        "        reference_opinion: np.ndarray | None = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_voters = int(num_voters)\n",
        "        self.num_districts = int(num_districts)\n",
        "        self.opinion_dim = int(opinion_dim)\n",
        "        self.horizon = int(horizon)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        # dynamics params\n",
        "        self.eta = float(eta)\n",
        "        self.a_thresh = float(a_thresh)\n",
        "        self.b_thresh = float(b_thresh)\n",
        "        self.mu_assim = float(mu_assim)\n",
        "        self.mu_backfire = float(mu_backfire)\n",
        "        self.rep_edge_weight = float(rep_edge_weight)\n",
        "\n",
        "        # target opinion c*\n",
        "        if reference_opinion is None:\n",
        "            self.c_star = np.zeros(self.opinion_dim, dtype=np.float32)\n",
        "        else:\n",
        "            self.c_star = np.asarray(reference_opinion, dtype=np.float32).reshape(self.opinion_dim)\n",
        "\n",
        "        # spaces\n",
        "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(self.num_voters, self.num_districts), dtype=np.float32)\n",
        "        # observation_space is symbolic; real obs is a PyG Data\n",
        "        self.observation_space = spaces.Dict({\n",
        "            \"x\": spaces.Box(low=-np.inf, high=np.inf, shape=(self.num_voters, self.opinion_dim), dtype=np.float32),\n",
        "            \"y\": spaces.Box(low=0, high=self.num_districts-1, shape=(self.num_voters,), dtype=np.int64),\n",
        "        })\n",
        "\n",
        "        # state\n",
        "        self.t = 0\n",
        "        self._x = None\n",
        "        self._pos = None\n",
        "        self._edge_index = None\n",
        "        self._edge_attr = None\n",
        "        self._assignment = None\n",
        "        self._y = None\n",
        "        self._reps = None\n",
        "\n",
        "\n",
        "    def reset(self, seed: int | None = None, options: dict | None = None):\n",
        "        super().reset(seed=seed)\n",
        "        self.t = 0\n",
        "\n",
        "        if options is None:\n",
        "            raise ValueError(\"reset(options=...) must provide 'opinions', 'pos', and either 'edge_index' or 'social_adj'.\")\n",
        "\n",
        "        x = np.asarray(options[\"opinions\"], dtype=np.float32)\n",
        "        pos = np.asarray(options[\"pos\"], dtype=np.float32)\n",
        "        if x.shape != (self.num_voters, self.opinion_dim):\n",
        "            raise ValueError(\"opinions shape mismatch\")\n",
        "        if pos.shape != (self.num_voters, 2):\n",
        "            raise ValueError(\"pos must be (N,2)\")\n",
        "\n",
        "        if \"edge_index\" in options:\n",
        "            edge_index = np.asarray(options[\"edge_index\"], dtype=np.int64)\n",
        "            edge_attr = np.asarray(options.get(\"edge_attr\", np.ones(edge_index.shape[1])), dtype=np.float32)\n",
        "        else:\n",
        "            adj = np.asarray(options[\"social_adj\"], dtype=np.float32)\n",
        "            edge_index, edge_attr = self._adj_to_coo(adj)\n",
        "\n",
        "        assignment = options.get(\"assignment\", np.full((self.num_voters, self.num_districts),\n",
        "                                                      1.0 / self.num_districts, dtype=np.float32))\n",
        "        assignment = self._row_normalize(assignment)\n",
        "        assignment = self._ensure_non_empty_soft(assignment)\n",
        "\n",
        "        y = assignment.argmax(axis=1).astype(np.int64)\n",
        "        reps = self._elect_representatives_from_labels(y, x)\n",
        "\n",
        "        # state\n",
        "        self._x, self._pos = x, pos\n",
        "        self._edge_index, self._edge_attr = edge_index, edge_attr\n",
        "        self._assignment, self._y, self._reps = assignment, y, reps\n",
        "\n",
        "        return self.as_pyg_data(), {}\n",
        "\n",
        "\n",
        "    def step(self, action: np.ndarray):\n",
        "        assignment = np.asarray(action, dtype=np.float32)\n",
        "        assignment = self._row_normalize(assignment)\n",
        "        assignment = self._ensure_non_empty_soft(assignment)\n",
        "\n",
        "        y = assignment.argmax(axis=1).astype(np.int64)\n",
        "        reps = self._elect_representatives_from_labels(y, self._x)\n",
        "\n",
        "        edge_index_aug, edge_attr_aug = self._augment_with_reps(self._edge_index, self._edge_attr, reps, y)\n",
        "        x_new = self._opinion_update(edge_index_aug, edge_attr_aug, self._x)\n",
        "        reward = self._reward(self._x, x_new)\n",
        "\n",
        "        # commit\n",
        "        self._assignment, self._y, self._reps = assignment, y, reps\n",
        "        self._x, self._edge_index, self._edge_attr = x_new, edge_index_aug, edge_attr_aug\n",
        "\n",
        "        self.t += 1\n",
        "        terminated = self.t >= self.horizon\n",
        "        return self.as_pyg_data(), float(reward), terminated, False, {}\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        mean = self._x.mean(axis=0)\n",
        "        print(f\"[t={self.t}] mean opinion ≈ {tuple(np.round(mean,3))}\")\n",
        "\n",
        "\n",
        "    def as_pyg_data(self) -> Data:\n",
        "        return Data(\n",
        "            x=torch.tensor(self._x, dtype=torch.float32),\n",
        "            y=torch.tensor(self._y, dtype=torch.long),\n",
        "            pos=torch.tensor(self._pos, dtype=torch.float32),\n",
        "            edge_index=torch.tensor(self._edge_index, dtype=torch.long),\n",
        "            edge_attr=torch.tensor(self._edge_attr, dtype=torch.float32),\n",
        "            assignment=torch.tensor(self._assignment, dtype=torch.float32),\n",
        "            reps=torch.tensor([r if r is not None else -1 for r in self._reps], dtype=torch.long),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _row_normalize(A: np.ndarray, eps=1e-8) -> np.ndarray:\n",
        "        row_sum = A.sum(axis=1, keepdims=True)\n",
        "        row_sum = np.clip(row_sum, eps, None)\n",
        "        return A / row_sum\n",
        "\n",
        "    def _ensure_non_empty_soft(self, A: np.ndarray) -> np.ndarray:\n",
        "        y = A.argmax(axis=1)\n",
        "        counts = np.bincount(y, minlength=self.num_districts)\n",
        "        for d in range(self.num_districts):\n",
        "            if counts[d] == 0:\n",
        "                donor = counts.argmax()\n",
        "                donor_nodes = np.where(y == donor)[0]\n",
        "                weakest = donor_nodes[np.argmin(A[donor_nodes, donor])]\n",
        "                A[weakest,:] = 0.0\n",
        "                A[weakest,d] = 1.0\n",
        "                y[weakest] = d\n",
        "                counts = np.bincount(y, minlength=self.num_districts)\n",
        "        return self._row_normalize(A)\n",
        "\n",
        "    @staticmethod\n",
        "    def _adj_to_coo(adj: np.ndarray):\n",
        "        row, col = np.nonzero(adj)\n",
        "        edge_index = np.vstack([row, col])\n",
        "        edge_attr = adj[row, col]\n",
        "        return edge_index, edge_attr\n",
        "\n",
        "    def _elect_representatives_from_labels(self, y: np.ndarray, X: np.ndarray):\n",
        "        reps = [None]*self.num_districts\n",
        "        dif = X[:,None,:] - X[None,:,:]\n",
        "        dists = np.linalg.norm(dif,axis=2)\n",
        "        for d in range(self.num_districts):\n",
        "            members = np.where(y==d)[0]\n",
        "            if len(members)==0: continue\n",
        "            sums = dists[np.ix_(members,members)].sum(axis=1)\n",
        "            reps[d] = int(members[np.argmin(sums)])\n",
        "        return reps\n",
        "\n",
        "    def _augment_with_reps(self, edge_index, edge_attr, reps, y):\n",
        "        add_src, add_dst, add_w = [], [], []\n",
        "        for d, r in enumerate(reps):\n",
        "            if r is None: continue\n",
        "            members = np.where(y==d)[0]\n",
        "            for v in members:\n",
        "                if v==r: continue\n",
        "                add_src.append(r)\n",
        "                add_dst.append(v)\n",
        "                add_w.append(self.rep_edge_weight)\n",
        "        if not add_src:\n",
        "            return edge_index, edge_attr\n",
        "        new_ei = np.concatenate([edge_index, np.vstack([add_src, add_dst])], axis=1)\n",
        "        new_ea = np.concatenate([edge_attr, np.array(add_w,dtype=np.float32)], axis=0)\n",
        "        return new_ei,new_ea\n",
        "\n",
        "    def _opinion_update(self, edge_index, edge_attr, X):\n",
        "        src,dst = edge_index\n",
        "        dif = X[src]-X[dst]\n",
        "        dist = np.linalg.norm(dif,axis=1)\n",
        "        gain = self._drf_gain(dist)\n",
        "        contrib = (gain*edge_attr)[:,None]*dif\n",
        "        delta = np.zeros_like(X)\n",
        "        np.add.at(delta,dst,contrib)\n",
        "        return X+self.eta*delta\n",
        "\n",
        "    def _drf_gain(self, d):\n",
        "        g=np.zeros_like(d,dtype=np.float32)\n",
        "        g[d<=self.a_thresh]=self.mu_assim\n",
        "        mid=(d>self.a_thresh)&(d<=self.b_thresh)\n",
        "        g[mid]=0.0\n",
        "        g[d>self.b_thresh]=self.mu_backfire\n",
        "        return g\n",
        "\n",
        "    def _reward(self, oldX,newX):\n",
        "        old_d=np.linalg.norm(oldX-self.c_star[None,:],axis=1).sum()\n",
        "        new_d=np.linalg.norm(newX-self.c_star[None,:],axis=1).sum()\n",
        "        return old_d-new_d\n",
        "\n"
      ],
      "metadata": {
        "id": "SVTh7C79jybC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mcmc_baseline(env, num_steps=10, check_steps=5, rng=None):\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "    current_assignment = np.eye(env.num_districts)[rng.integers(0, env.num_districts, size=env.num_voters)]\n",
        "    current_y = current_assignment.argmax(axis=1)\n",
        "    reps = env._elect_representatives_from_labels(current_y, env._x)\n",
        "    edge_index_aug, edge_attr_aug = env._augment_with_reps(env._edge_index, env._edge_attr, reps, current_y)\n",
        "    x_new = env._opinion_update(edge_index_aug, edge_attr_aug, env._x)\n",
        "    current_reward = env._reward(env._x, x_new)\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        proposal = current_assignment.copy()\n",
        "        voter = rng.integers(0, env.num_voters)\n",
        "        new_d = rng.integers(0, env.num_districts)\n",
        "        proposal[voter, :] = 0\n",
        "        proposal[voter, new_d] = 1.0\n",
        "        proposal_y = proposal.argmax(axis=1)\n",
        "\n",
        "        check_assignment, check_reward = run_mini_chain(env, proposal, check_steps, rng)\n",
        "\n",
        "        if check_reward > current_reward:\n",
        "            current_assignment, current_y, current_reward = check_assignment, proposal_y, check_reward\n",
        "\n",
        "    return current_assignment, current_y\n"
      ],
      "metadata": {
        "id": "cttpLix5rA_J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_mini_chain(env, start_assignment, check_steps, rng):\n",
        "    check_assignment = start_assignment.copy()\n",
        "    check_reward = -np.inf\n",
        "\n",
        "    current_y = check_assignment.argmax(axis=1)\n",
        "    reps = env._elect_representatives_from_labels(current_y, env._x)\n",
        "    edge_index_aug, edge_attr_aug = env._augment_with_reps(env._edge_index, env._edge_attr, reps, current_y)\n",
        "    x_new = env._opinion_update(edge_index_aug, edge_attr_aug, env._x)\n",
        "    check_reward = env._reward(env._x, x_new)\n",
        "\n",
        "    for _ in range(check_steps):\n",
        "        temp = check_assignment.copy()\n",
        "        v = rng.integers(0, env.num_voters)\n",
        "        d = rng.integers(0, env.num_districts)\n",
        "        temp[v, :] = 0\n",
        "        temp[v, d] = 1.0\n",
        "\n",
        "        temp_y = temp.argmax(axis=1)\n",
        "        reps = env._elect_representatives_from_labels(temp_y, env._x)\n",
        "        edge_index_aug, edge_attr_aug = env._augment_with_reps(env._edge_index, env._edge_attr, reps, temp_y)\n",
        "        x_new = env._opinion_update(edge_index_aug, edge_attr_aug, env._x)\n",
        "        temp_reward = env._reward(env._x, x_new)\n",
        "\n",
        "        if temp_reward > check_reward:\n",
        "            check_assignment, check_reward = temp, temp_reward\n",
        "\n",
        "    return check_assignment, check_reward\n"
      ],
      "metadata": {
        "id": "_FoJYH7PLeS7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 16          # voters arranged in 1D line\n",
        "K = 4           # 4 districts\n",
        "T = 8\n",
        "\n",
        "opinions = np.linspace(-1,1,N)[:,None]\n",
        "opinions = np.hstack([opinions, np.zeros((N,1))])  # 2D opinions\n",
        "pos = np.arange(N)[:,None]\n",
        "pos = np.hstack([pos, np.zeros_like(pos)])          # (N,2)\n",
        "edges = [(i,i+1) for i in range(N-1)]\n",
        "edge_index = np.array(edges + [(j,i) for i,j in edges]).T\n",
        "edge_attr = np.ones(edge_index.shape[1])\n",
        "\n",
        "env = FrankenmanderingEnv(num_voters=N, num_districts=K, opinion_dim=2, horizon=T)\n",
        "\n",
        "obs, _ = env.reset(options={\n",
        "    \"opinions\": opinions,\n",
        "    \"pos\": pos,\n",
        "    \"edge_index\": edge_index,\n",
        "    \"edge_attr\": edge_attr\n",
        "})\n",
        "\n",
        "for t in range(T):\n",
        "    assignment, y = mcmc_baseline(env, num_steps=10)\n",
        "    obs, reward, done, _, _ = env.step(assignment)\n",
        "    print(f\"t={t}, reward={reward:.3f}, mean opinion={obs.x.mean(0)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdqo0LSLlnDY",
        "outputId": "f926835b-6746-4e69-c3cd-f73e70901448"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t=0, reward=0.453, mean opinion=tensor([0.0050, 0.0000])\n",
            "t=1, reward=0.741, mean opinion=tensor([0.0013, 0.0000])\n",
            "t=2, reward=0.878, mean opinion=tensor([-0.0127,  0.0000])\n",
            "t=3, reward=0.819, mean opinion=tensor([-0.0190,  0.0000])\n",
            "t=4, reward=0.704, mean opinion=tensor([-0.0171,  0.0000])\n",
            "t=5, reward=0.644, mean opinion=tensor([-0.0354,  0.0000])\n",
            "t=6, reward=0.832, mean opinion=tensor([-0.0385,  0.0000])\n",
            "t=7, reward=1.211, mean opinion=tensor([-0.0464,  0.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_inchworm():\n",
        "    N = 16\n",
        "    K = 4\n",
        "    T = 8\n",
        "\n",
        "    opinions = np.linspace(-1,1,N)[:,None]\n",
        "    opinions = np.hstack([opinions, np.zeros((N,1))])\n",
        "    pos = np.arange(N)[:,None]\n",
        "    pos = np.hstack([pos, np.zeros_like(pos)])\n",
        "\n",
        "    edges = [(i,i+1) for i in range(N-1)]\n",
        "    edge_index = np.array(edges+[(j,i) for i,j in edges]).T\n",
        "    edge_attr = np.ones(edge_index.shape[1])\n",
        "\n",
        "    env = FrankenmanderingEnv(num_voters=N,num_districts=K,opinion_dim=2,horizon=T)\n",
        "    obs,_=env.reset(options={\"opinions\":opinions,\"pos\":pos,\"edge_index\":edge_index,\"edge_attr\":edge_attr})\n",
        "\n",
        "    for t in range(T):\n",
        "        assignment = np.zeros((N,K))\n",
        "        block = N//K\n",
        "        for d in range(K):\n",
        "            start=(d+ t)%N\n",
        "            end=(start+block)%N\n",
        "            if start<end:\n",
        "                assignment[start:end,d]=1.0\n",
        "            else:\n",
        "                assignment[start:,d]=1.0\n",
        "                assignment[:end,d]=1.0\n",
        "        obs,reward,done,_,_=env.step(assignment)\n",
        "        print(f\"t={t}, reward={reward:.3f}, mean opinion={obs.x.mean(0)}\")\n"
      ],
      "metadata": {
        "id": "zy5uuWD6yggR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inch:\n",
        "# t=0, reward=0.160, mean opinion=tensor([3.7253e-09, 0.0000e+00])\n",
        "# t=1, reward=0.144, mean opinion=tensor([-0.0027,  0.0000])\n",
        "# t=2, reward=0.377, mean opinion=tensor([-0.0211,  0.0000])\n",
        "# t=3, reward=0.273, mean opinion=tensor([-0.0218,  0.0000])\n",
        "# t=4, reward=0.582, mean opinion=tensor([-0.0342,  0.0000])\n",
        "# t=5, reward=0.356, mean opinion=tensor([-0.0346,  0.0000])\n",
        "# t=6, reward=0.700, mean opinion=tensor([-0.0535,  0.0000])\n",
        "# t=7, reward=0.459, mean opinion=tensor([-0.0633,  0.0000])"
      ],
      "metadata": {
        "id": "P2w-e05REqUV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_inchworm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn7H0QXKzULI",
        "outputId": "96e0c392-5302-4f43-c6cf-ee1918a4eada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t=0, reward=0.160, mean opinion=tensor([3.7253e-09, 0.0000e+00])\n",
            "t=1, reward=0.144, mean opinion=tensor([-0.0027,  0.0000])\n",
            "t=2, reward=0.377, mean opinion=tensor([-0.0211,  0.0000])\n",
            "t=3, reward=0.273, mean opinion=tensor([-0.0218,  0.0000])\n",
            "t=4, reward=0.582, mean opinion=tensor([-0.0342,  0.0000])\n",
            "t=5, reward=0.356, mean opinion=tensor([-0.0346,  0.0000])\n",
            "t=6, reward=0.700, mean opinion=tensor([-0.0535,  0.0000])\n",
            "t=7, reward=0.459, mean opinion=tensor([-0.0633,  0.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def nx_to_coo(G):\n",
        "    edge_index = np.array(list(G.edges)).T\n",
        "    edge_index = np.concatenate([edge_index, edge_index[::-1]], axis=1)\n",
        "    edge_attr = np.ones(edge_index.shape[1], dtype=np.float32)\n",
        "    return edge_index, edge_attr\n",
        "\n",
        "def run_social_test(graph_type=\"BA\", N=50, K=5, horizon=10, seed=0):\n",
        "    if graph_type == \"BA\":\n",
        "        G = nx.barabasi_albert_graph(N, m=3, seed=seed)\n",
        "    elif graph_type == \"NWS\":\n",
        "        G = nx.newman_watts_strogatz_graph(N, k=6, p=0.1, seed=seed)\n",
        "    else:\n",
        "        raise ValueError(\"graph_type must be 'BA' or 'NWS'\")\n",
        "\n",
        "    edge_index, edge_attr = nx_to_coo(G)\n",
        "\n",
        "    opinions = np.random.uniform(-1, 1, size=(N, 2))\n",
        "    pos = np.random.uniform(-1, 1, size=(N, 2))\n",
        "\n",
        "    env = FrankenmanderingEnv(num_voters=N, num_districts=K, horizon=horizon, seed=seed)\n",
        "    obs, _ = env.reset(options={\n",
        "        \"opinions\": opinions,\n",
        "        \"pos\": pos,\n",
        "        \"edge_index\": edge_index,\n",
        "        \"edge_attr\": edge_attr\n",
        "    })\n",
        "\n",
        "    for t in range(horizon):\n",
        "        action = np.random.dirichlet([1.0]*K, size=N)  # soft district assignment\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        mean_opinion = torch.tensor(env._x).mean(dim=0)\n",
        "        print(f\"[{graph_type}] t={t}, reward={reward:.3f}, mean_opinion={mean_opinion}\")\n",
        "        if terminated: break\n",
        "\n",
        "print(\"\\n=== Barabási–Albert Graph Test ===\")\n",
        "run_social_test(\"BA\")\n",
        "\n",
        "print(\"\\n=== Newman–Watts–Strogatz Graph Test ===\")\n",
        "run_social_test(\"NWS\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_RwPM4y2Buj",
        "outputId": "9307eeb5-b541-453a-95af-3788eb6d99fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Barabási–Albert Graph Test ===\n",
            "[BA] t=0, reward=-11.333, mean_opinion=tensor([ 0.0306, -0.0770])\n",
            "[BA] t=1, reward=-31.712, mean_opinion=tensor([ 0.0226, -0.0633])\n",
            "[BA] t=2, reward=-91.064, mean_opinion=tensor([-0.0016, -0.0453])\n",
            "[BA] t=3, reward=-270.099, mean_opinion=tensor([-0.1713,  0.1011])\n",
            "[BA] t=4, reward=-870.752, mean_opinion=tensor([-0.8037,  0.6898])\n",
            "[BA] t=5, reward=-2981.641, mean_opinion=tensor([-2.8489,  2.9611])\n",
            "[BA] t=6, reward=-10712.967, mean_opinion=tensor([-11.0578,  13.0435])\n",
            "[BA] t=7, reward=-40060.582, mean_opinion=tensor([-38.9094,  49.8956])\n",
            "[BA] t=8, reward=-155089.188, mean_opinion=tensor([-146.2493,  198.3502])\n",
            "[BA] t=9, reward=-616487.125, mean_opinion=tensor([-605.9678,  852.0072])\n",
            "\n",
            "=== Newman–Watts–Strogatz Graph Test ===\n",
            "[NWS] t=0, reward=-29.737, mean_opinion=tensor([-0.0543, -0.0071])\n",
            "[NWS] t=1, reward=-63.545, mean_opinion=tensor([-0.0342, -0.0228])\n",
            "[NWS] t=2, reward=-142.651, mean_opinion=tensor([-0.0526, -0.0180])\n",
            "[NWS] t=3, reward=-328.061, mean_opinion=tensor([-0.1029,  0.0240])\n",
            "[NWS] t=4, reward=-786.363, mean_opinion=tensor([-0.0391,  0.0893])\n",
            "[NWS] t=5, reward=-1972.454, mean_opinion=tensor([-0.3201,  0.5718])\n",
            "[NWS] t=6, reward=-5158.318, mean_opinion=tensor([-1.5247,  1.7711])\n",
            "[NWS] t=7, reward=-14017.004, mean_opinion=tensor([-5.7884,  5.3240])\n",
            "[NWS] t=8, reward=-39679.547, mean_opinion=tensor([-12.5134,  11.3532])\n",
            "[NWS] t=9, reward=-116614.656, mean_opinion=tensor([-26.1567,  23.2505])\n"
          ]
        }
      ]
    }
  ]
}