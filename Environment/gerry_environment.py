# -*- coding: utf-8 -*-
"""Gerry_Environment_30.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nymLff-LAeRK034FGpFrKOmmz31t7Yll
"""

# !pip install torch_geometric

"""##**Gerrymandering-Environment**

    INITIAL STATE (provided externally via reset(options=...)):
        - 'district_map'
        - 'social_graph'
        - 'opinions'     

    ACTION:
        - new district assignment for each voter

    OBSERVATION (returned by reset/step):
        {
          'district_map'   : (num_voters,)
          'representatives': (num_districts,)  # voter indices; -1 if empty   district
          'social_graph'   : (num_voters, num_voters)  # AUGMENTED: base social + rep->voter edges used for the step
          'opinions'       : (num_voters, 2)
          'opinion_graph'  : (num_voters, num_voters)  # similarity kernel derived from opinion distances
        }

    KEY LOGIC:
      - Representatives: for each district, pick the member that minimizes the sum of L2 distances to members in that district (discrete 1-median).
      - Opinion dynamics: DRF (assimilation/neutral/backfire) with weighted neighbor influence.
      - Reward: reduction in total distance to reference opinion c*

#This is the Frankenmandering Data class which includes:
*   social edge list: type: matrix and size : (v,v)
*   assignment edge list: type: matrix and size:(v,d)
*   number of orignin edge
*   geometric position
*   district label
*   edge attribute
*   representative
*   opinion
"""



import numpy as np
import torch
from torch_geometric.data import Data
import gymnasium as gym
from gymnasium import spaces
from typing import Optional, Tuple, Dict, Any
import math
from helpers_functions import *

class FrankenData(Data):
    def __init__(self, edge_index, geographical_edge, orig_edge_num,
          x, pos, reps, y, edge_attr, geo_edge_attr, **kwargs):

        super().__init__(
            edge_index = torch.as_tensor(edge_index, dtype=torch.long),
            x = torch.as_tensor(x, dtype=torch.float32),
            pos = torch.as_tensor(pos, dtype=torch.float32),
            y =  torch.as_tensor(y, dtype=torch.long),
            edge_attr = torch.as_tensor(edge_attr, dtype=torch.float32),
            **kwargs
        )

        self._orig_edge_num = orig_edge_num
        self._geographical_edge = torch.as_tensor(geographical_edge, dtype=torch.long)
        self._geo_edge_attr = torch.as_tensor(geo_edge_attr, dtype=torch.float32)
        self._reps = reps

    def get_number(self):
          return self._orig_edge_num

    def get_edge_index(self):
          return self.edge_index

    def get_edge_attr(self):
          return self.edge_attr

    def get_x(self):
          return self.x

    def get_geo_edge(self):
          return self._geographical_edge

    def get_pos(self):
          return self.pos

    def get_geo_edge_attr(self):
      return self._geo_edge_attr

    def get_y(self):
      return self.y



class FrankenmanderingEnv(gym.Env):

    metadata = {"render_modes": ["human"]}

    def __init__(
        self,
        num_voters: int,
        num_districts: int,
        opinion_dim: int = 2,
        horizon: int = 10,
        seed: int | None = None,
        init_FrankenData : FrankenData | None = None,
        target_opinion: np.ndarray | None = None
    ):
        super().__init__()
        self._num_voters = num_voters
        self._num_districts = num_districts
        self._opinion_dim = opinion_dim
        self._horizon = horizon
        self._rng = np.random.default_rng(seed)

        self._t = 0
        self._initial_data = init_FrankenData
        self._FrankenData = None
        # self.__opinion_update_step = 0

        self._transition_history = []
        self._committed_state = None
        self._c_star = target_opinion
        # target opinion c*
        # if target_opinion is None:
        #     self._c_star = np.zeros((self._num_voters,self._opinion_dim), dtype=np.float32)
        # else:
        #     self._c_star = np.asarray(target_opinion, dtype=np.float32).reshape(self._num_voters,self._opinion_dim)

        # action spaces
        self._action_space = spaces.Box(low=0.0, high=1.0, shape=(self._num_voters, self._num_districts), dtype=np.float32)

        self._observation_space = spaces.Dict({
                  "opinions": spaces.Box(low=-np.inf, high=np.inf,
                                        shape=(self._num_voters, self._opinion_dim), dtype=np.float32),

                  "positions": spaces.Box(low=-np.inf, high=np.inf,
                                          shape=(self._num_voters, 2), dtype=np.float32),

                  "dist_label": spaces.Box(low=0, high=self._num_districts-1,
                                          shape=(self._num_voters,), dtype=np.int32),

                  "reps": spaces.Box(low=-1, high=self._num_voters-1,
                                    shape=(self._num_districts,), dtype=np.int32),

                  "social_edge_index": spaces.Box(low=0, high=self._num_voters-1,
                                shape=(2, self._initial_data.get_number()), dtype=np.int32),

                  "social_edge_attr": spaces.Box(low=0, high=np.inf,
                                                shape=(self._initial_data.get_number(),), dtype=np.float32),

                  "assignment": spaces.Box(low=0, high=1,
                                          shape=(self._num_voters, self._num_districts), dtype=np.int32),
              })


    def reset(self, seed: int | None = None, options: dict | None = None, FrankenData = None):

        super().reset(seed=seed)

        if FrankenData is None:
          FrankenData = self._initial_data
          self._t = 0

        self._FrankenData = FrankenData
        self._committed_state = FrankenData
        self._transition_history = []

        self._transition_history.append(self._committed_state)

        return self._FrankenData, {}

    def step(self, action: np.ndarray, DRF):

        if self._FrankenData is None:
            raise RuntimeError("Environment must be reset before calling step.")

        # Normalize action and compute new assignment
        assignment = row_normalize(np.asarray(action, dtype=np.float32))

        dist_label = assignment.argmax(axis=1).astype(np.int32)

        # Elect new representatives
        reps = elect_representatives(dist_label, self._FrankenData.get_x(), self._num_districts)

        #partial reset augmented graph
        social_edge = self._FrankenData.get_edge_index()
        orig_num = self._FrankenData.get_number()
        social_graph = social_edge[:, :orig_num]

        edge_attr = self._FrankenData.get_edge_attr()
        social_edge_attr = edge_attr[:orig_num]

        # update social graph based on reps and new assignment
        aug_edge_index, aug_edge_attr = augment_with_reps(
            social_graph,
            social_edge_attr,
            reps,
            dist_label
        )

        # Update oppinion
        x = self._FrankenData.get_x()
        x_new = opinion_update(aug_edge_index, aug_edge_attr,x, DRF)

        geographical_edge = self._FrankenData.get_geo_edge()
        reward = self.reward(x_new)

        self._t += 1
        # self.opinion_update_step +=1
        terminated = self._t >= self._horizon

        self._FrankenData = FrankenData(
              edge_index = aug_edge_index,
              orig_edge_num = self._initial_data.get_number(),
              geographical_edge = self._initial_data.get_geo_edge(),
              x= x_new,
              pos = self._initial_data.get_pos(),
              reps =[r if r is not None else -1 for r in reps],
              y = dist_label,
              edge_attr = aug_edge_attr,
              geo_edge_attr = self._initial_data.get_geo_edge_attr()
            )

        self._transition_history.append(self._FrankenData)

        return self._FrankenData, float(reward), terminated, False, {}

    def reward(self, new_opinion):
        # check the population equity and compactness constraints
        # pop_dev = population_equality(np.ones_like(dist_label),dist_label, self.num_districts)
        # comp_score = compactness_score(geo_edge, dist_label)

        # penalty =  -Beta1 * pop_dev - Beta2 * comp_score

        # old_d = np.linalg.norm(old_opinion-self._initial_data,axis=1).sum()
        init_ops = np.array(self._initial_data.get_x())
        new_d = np.linalg.norm(new_opinion-init_ops,axis=1).sum()

        # reward = penalty + old_d-new_d

        return new_d

    def commit(self, committed_state):

      self._FrankenData = committed_state
      self._committed_state = committed_state
      self._transition_history.clear()
      self._transition_history.append(committed_state)

      return self._transition_history

    def get_commited_state(self):
      return self._committed_state

    def get_num_districts(self):
      return self._num_districts

    def set_num_districts(self,num_districts ):
        self._num_districts = num_districts

    def get_init_data(self):
      return self._initial_data

    def get_target(self):
      return self._c_star

    def get_num_voters(self):
      return self._num_voters